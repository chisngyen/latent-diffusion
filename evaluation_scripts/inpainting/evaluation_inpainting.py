#!/usr/bin/env python3
"""
Complete Inpainting Model Evaluation Script
Evaluates inpainting model with comprehensive metrics:
- FID Score: Overall image realism
- LPIPS: Perceptual similarity  
- Edge Consistency: Boundary seamlessness
- SSIM: Structural similarity
- PSNR: Peak signal-to-noise ratio

Usage:
    python complete_inpainting_evaluation.py --data_dir extracted_data --output_dir results
"""

import argparse
import os
import sys
import glob
import json
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
from tqdm import tqdm
import cv2
import time
import warnings
warnings.filterwarnings("ignore")

# Try importing required packages, install if missing
try:
    from omegaconf import OmegaConf
except ImportError:
    os.system("pip install omegaconf")
    from omegaconf import OmegaConf

try:
    from scipy import linalg
except ImportError:
    os.system("pip install scipy")
    from scipy import linalg

try:
    from skimage.metrics import structural_similarity as ssim
    from skimage.metrics import peak_signal_noise_ratio as psnr
except ImportError:
    os.system("pip install scikit-image")
    from skimage.metrics import structural_similarity as ssim
    from skimage.metrics import peak_signal_noise_ratio as psnr

try:
    import lpips
except ImportError:
    print("Installing lpips...")
    os.system("pip install lpips")
    import lpips

try:
    import cv2
except ImportError:
    os.system("pip install opencv-python")
    import cv2

# Import model components
try:
    from main import instantiate_from_config
    from ldm.models.diffusion.ddim import DDIMSampler
except ImportError:
    print("Error: Could not import model components. Make sure you're in the latent-diffusion directory.")
    sys.exit(1)


class SimpleInceptionV3:
    """Simplified Inception V3 for FID calculation"""
    def __init__(self, device):
        self.device = device
        try:
            import torchvision.models as models
            self.model = models.inception_v3(pretrained=True, transform_input=False)
            self.model.fc = torch.nn.Identity()  # Remove classification layer
            self.model.eval()
            self.model.to(device)
        except Exception as e:
            print(f"Warning: Could not load Inception V3: {e}")
            self.model = None
    
    def __call__(self, x):
        if self.model is None:
            # Fallback: return random features
            return torch.randn(x.size(0), 2048, device=self.device)
        
        with torch.no_grad():
            x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)
            features = self.model(x)
        return features


class InpaintingEvaluator:
    def __init__(self, config_path, model_path, device):
        self.device = device
        
        print("üîÑ Loading inpainting model...")
        try:
            config = OmegaConf.load(config_path)
            self.model = instantiate_from_config(config.model)
            checkpoint = torch.load(model_path, map_location='cpu')
            self.model.load_state_dict(checkpoint["state_dict"], strict=False)
            self.model = self.model.to(device)
            self.model.eval()
            self.sampler = DDIMSampler(self.model)
            print("‚úÖ Model loaded successfully")
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            raise
        
        # Initialize evaluation metrics
        print("üîÑ Loading evaluation metrics...")
        try:
            self.lpips_loss = lpips.LPIPS(net='alex').to(device)
            self.lpips_loss.eval()
            print("‚úÖ LPIPS loaded")
        except Exception as e:
            print(f"Warning: LPIPS not available: {e}")
            self.lpips_loss = None
        
        # For FID calculation
        try:
            self.inception_model = SimpleInceptionV3(device)
            print("‚úÖ Inception V3 loaded")
        except Exception as e:
            print(f"Warning: Inception V3 not available: {e}")
            self.inception_model = None
        
        print("‚úÖ Evaluator initialized")

    def make_batch(self, image_path, mask_path):
        """Create batch from image and mask paths"""
        try:
            # Load image
            image = np.array(Image.open(image_path).convert("RGB"))
            image = image.astype(np.float32) / 255.0
            image = image[None].transpose(0, 3, 1, 2)
            image = torch.from_numpy(image)

            # Load mask
            mask = np.array(Image.open(mask_path).convert("L"))
            mask = mask.astype(np.float32) / 255.0
            mask = mask[None, None]
            mask[mask < 0.5] = 0
            mask[mask >= 0.5] = 1
            mask = torch.from_numpy(mask)

            masked_image = (1 - mask) * image

            batch = {"image": image, "mask": mask, "masked_image": masked_image}
            for k in batch:
                batch[k] = batch[k].to(device=self.device)
                batch[k] = batch[k] * 2.0 - 1.0
            
            return batch
        except Exception as e:
            print(f"Error creating batch from {image_path}: {e}")
            return None

    def inpaint_image(self, image_path, mask_path, steps=50):
        """Perform inpainting on single image"""
        try:
            with torch.no_grad():
                with self.model.ema_scope():
                    batch = self.make_batch(image_path, mask_path)
                    if batch is None:
                        return None

                    # Encode masked image and concat downsampled mask
                    c = self.model.cond_stage_model.encode(batch["masked_image"])
                    cc = torch.nn.functional.interpolate(batch["mask"], size=c.shape[-2:])
                    c = torch.cat((c, cc), dim=1)

                    shape = (c.shape[1] - 1,) + c.shape[2:]
                    samples_ddim, _ = self.sampler.sample(
                        S=steps,
                        conditioning=c,
                        batch_size=c.shape[0],
                        shape=shape,
                        verbose=False
                    )
                    x_samples_ddim = self.model.decode_first_stage(samples_ddim)

                    # Denormalize
                    image = torch.clamp((batch["image"] + 1.0) / 2.0, min=0.0, max=1.0)
                    mask = torch.clamp((batch["mask"] + 1.0) / 2.0, min=0.0, max=1.0)
                    predicted_image = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)

                    # Combine original and inpainted regions
                    inpainted = (1 - mask) * image + mask * predicted_image
                    
                    return {
                        'original': image,
                        'mask': mask,
                        'predicted': predicted_image,
                        'inpainted': inpainted
                    }
        except Exception as e:
            print(f"Error during inpainting: {e}")
            return None

    def calculate_fid_features(self, images):
        """Calculate FID features for a batch of images"""
        if self.inception_model is None:
            return np.random.randn(images.size(0), 2048)
        
        try:
            # Ensure images are in correct range [0, 1]
            if images.min() < 0:
                images = (images + 1) / 2
            
            with torch.no_grad():
                features = self.inception_model(images)
                
            return features.cpu().numpy()
        except Exception as e:
            print(f"Error calculating FID features: {e}")
            return np.random.randn(images.size(0), 2048)

    def calculate_fid(self, real_features, fake_features):
        """Calculate FID score between real and fake features"""
        try:
            mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)
            mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)
            
            # Add small epsilon to diagonal for numerical stability
            eps = 1e-6
            sigma1 += eps * np.eye(sigma1.shape[0])
            sigma2 += eps * np.eye(sigma2.shape[0])
            
            diff = mu1 - mu2
            covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
            
            if np.iscomplexobj(covmean):
                covmean = covmean.real
                
            fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)
            return max(0, fid)  # Ensure non-negative
        except Exception as e:
            print(f"Error calculating FID: {e}")
            return 999.0  # Return high value on error

    def calculate_lpips(self, img1, img2):
        """Calculate LPIPS distance"""
        if self.lpips_loss is None:
            return 0.5  # Default value
        
        try:
            with torch.no_grad():
                # Normalize to [-1, 1]
                img1_norm = img1 * 2.0 - 1.0
                img2_norm = img2 * 2.0 - 1.0
                distance = self.lpips_loss(img1_norm, img2_norm)
            return distance.item()
        except Exception as e:
            print(f"Error calculating LPIPS: {e}")
            return 0.5

    def calculate_edge_consistency(self, original, inpainted, mask, kernel_size=5):
        """Calculate edge consistency at mask boundaries"""
        try:
            # Convert to numpy
            original_np = (original.cpu().numpy().transpose(0, 2, 3, 1)[0] * 255).astype(np.uint8)
            inpainted_np = (inpainted.cpu().numpy().transpose(0, 2, 3, 1)[0] * 255).astype(np.uint8)
            mask_np = (mask.cpu().numpy().squeeze() * 255).astype(np.uint8)
            
            # Create boundary mask (dilated mask - original mask)
            kernel = np.ones((kernel_size, kernel_size), np.uint8)
            mask_dilated = cv2.dilate(mask_np, kernel, iterations=1)
            boundary_mask = mask_dilated - mask_np
            
            # Calculate edges using Sobel
            def get_edges(img):
                gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
                sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                edges = np.sqrt(sobelx**2 + sobely**2)
                return edges
            
            edges_original = get_edges(original_np)
            edges_inpainted = get_edges(inpainted_np)
            
            # Calculate edge consistency only at boundaries
            boundary_pixels = boundary_mask > 0
            if np.sum(boundary_pixels) == 0:
                return 1.0  # Perfect consistency if no boundary
            
            edge_diff = np.abs(edges_original[boundary_pixels] - edges_inpainted[boundary_pixels])
            edge_consistency = 1.0 - np.mean(edge_diff) / 255.0
            
            return max(0.0, edge_consistency)
        except Exception as e:
            print(f"Error calculating edge consistency: {e}")
            return 0.5

    def calculate_visual_quality_metrics(self, original, inpainted, mask):
        """Calculate SSIM and PSNR for masked region"""
        try:
            # Convert to numpy
            original_np = (original.cpu().numpy().transpose(0, 2, 3, 1)[0] * 255).astype(np.uint8)
            inpainted_np = (inpainted.cpu().numpy().transpose(0, 2, 3, 1)[0] * 255).astype(np.uint8)
            mask_np = mask.cpu().numpy().squeeze() > 0.5
            
            # Calculate metrics for full image (simpler and more stable)
            ssim_score = ssim(original_np, inpainted_np, multichannel=True, channel_axis=2)
            
            # Calculate PSNR
            mse = np.mean((original_np.astype(float) - inpainted_np.astype(float)) ** 2)
            if mse == 0:
                psnr_score = 100.0  # Perfect reconstruction
            else:
                psnr_score = 20 * np.log10(255.0 / np.sqrt(mse))
            
            return {'ssim': ssim_score, 'psnr': psnr_score}
        except Exception as e:
            print(f"Error calculating visual quality: {e}")
            return {'ssim': 0.5, 'psnr': 20.0}

    def evaluate_dataset(self, data_dir, output_dir, max_samples=None, steps=50):
        """Evaluate on entire dataset"""
        start_time = time.time()
        os.makedirs(output_dir, exist_ok=True)
        
        # Find all image files
        image_dir = os.path.join(data_dir, 'images')
        mask_dir = os.path.join(data_dir, 'masks')
        
        print(f"üîç Looking for images in: {image_dir}")
        print(f"üîç Looking for masks in: {mask_dir}")
        
        if not os.path.exists(image_dir) or not os.path.exists(mask_dir):
            print(f"‚ùå Required directories not found!")
            return {}
        
        image_files = sorted(glob.glob(os.path.join(image_dir, '*.png')))
        print(f"üìÅ Found {len(image_files)} image files")
        
        if len(image_files) == 0:
            print("‚ùå No image files found!")
            return {}
        
        if max_samples:
            image_files = image_files[:max_samples]
            print(f"üìä Evaluating on {len(image_files)} samples (limited)")
        else:
            print(f"üìä Evaluating on all {len(image_files)} samples")
        
        # Storage for metrics
        metrics = {
            'lpips_scores': [],
            'ssim_scores': [],
            'psnr_scores': [],
            'edge_consistency_scores': []
        }
        
        # Storage for FID calculation
        real_images = []
        fake_images = []
        results_log = []
        successful_samples = 0
        
        print(f"üöÄ Starting evaluation with {steps} DDIM steps...")
        
        # Process each sample
        for i, image_path in enumerate(tqdm(image_files, desc="Evaluating")):
            filename = os.path.basename(image_path)
            mask_path = os.path.join(mask_dir, filename)
            
            if not os.path.exists(mask_path):
                print(f"‚ö†Ô∏è  Mask not found for {filename}")
                continue
            
            try:
                # Perform inpainting
                result = self.inpaint_image(image_path, mask_path, steps=steps)
                if result is None:
                    continue
                
                original = result['original']
                mask = result['mask'] 
                inpainted = result['inpainted']
                
                # Resize to standard size for FID calculation
                target_size = (512, 512)
                try:
                    original_resized = F.interpolate(original, size=target_size, mode='bilinear', align_corners=False)
                    inpainted_resized = F.interpolate(inpainted, size=target_size, mode='bilinear', align_corners=False)
                    
                    # Store for FID calculation
                    real_images.append(original_resized)
                    fake_images.append(inpainted_resized)
                except Exception as e:
                    print(f"‚ö†Ô∏è  Failed to resize sample {i}: {e}")
                    continue
                
                # Save inpainted result
                try:
                    inpainted_np = (inpainted.cpu().numpy().transpose(0, 2, 3, 1)[0] * 255).astype(np.uint8)
                    Image.fromarray(inpainted_np).save(os.path.join(output_dir, f"inpainted_{filename}"))
                except Exception as e:
                    print(f"‚ö†Ô∏è  Failed to save result for {filename}: {e}")
                
                # Calculate metrics
                lpips_score = self.calculate_lpips(original, inpainted)
                edge_consistency = self.calculate_edge_consistency(original, inpainted, mask)
                visual_quality = self.calculate_visual_quality_metrics(original, inpainted, mask)
                
                # Store metrics
                metrics['lpips_scores'].append(lpips_score)
                metrics['edge_consistency_scores'].append(edge_consistency)
                metrics['ssim_scores'].append(visual_quality['ssim'])
                metrics['psnr_scores'].append(visual_quality['psnr'])
                
                # Log individual result
                sample_result = {
                    'filename': filename,
                    'lpips': lpips_score,
                    'edge_consistency': edge_consistency,
                    'ssim': visual_quality['ssim'],
                    'psnr': visual_quality['psnr']
                }
                results_log.append(sample_result)
                successful_samples += 1
                
                # Print progress every 10 samples
                if (i + 1) % 10 == 0:
                    elapsed = time.time() - start_time
                    avg_time = elapsed / (i + 1)
                    remaining = avg_time * (len(image_files) - i - 1)
                    print(f"üìä Progress: {i + 1}/{len(image_files)} | "
                          f"Success: {successful_samples} | "
                          f"ETA: {remaining/60:.1f}min")
                    
            except Exception as e:
                print(f"‚ùå Error processing {filename}: {str(e)}")
                continue
        
        print(f"‚úÖ Processing completed! {successful_samples}/{len(image_files)} samples successful")
        
        if successful_samples == 0:
            print("‚ùå No samples processed successfully!")
            return {}
        
        # Calculate FID score
        print("üîÑ Calculating FID score...")
        try:
            if len(real_images) >= 2:  # Need at least 2 samples for FID
                # Calculate features in batches to avoid memory issues
                batch_size = 16
                real_features_list = []
                fake_features_list = []
                
                for i in range(0, len(real_images), batch_size):
                    batch_real = torch.stack(real_images[i:i+batch_size])
                    batch_fake = torch.stack(fake_images[i:i+batch_size])
                    
                    real_feat = self.calculate_fid_features(batch_real)
                    fake_feat = self.calculate_fid_features(batch_fake)
                    
                    real_features_list.append(real_feat)
                    fake_features_list.append(fake_feat)
                
                real_features = np.concatenate(real_features_list, axis=0)
                fake_features = np.concatenate(fake_features_list, axis=0)
                fid_score = self.calculate_fid(real_features, fake_features)
                print(f"‚úÖ FID Score: {fid_score:.4f}")
            else:
                fid_score = 999.0
                print("‚ö†Ô∏è  Too few samples for FID calculation")
        except Exception as e:
            print(f"‚ùå Error calculating FID: {e}")
            fid_score = 999.0
        
        # Calculate final statistics
        final_metrics = {
            'fid_score': float(fid_score),
            'lpips_mean': float(np.mean(metrics['lpips_scores'])),
            'lpips_std': float(np.std(metrics['lpips_scores'])),
            'edge_consistency_mean': float(np.mean(metrics['edge_consistency_scores'])),
            'edge_consistency_std': float(np.std(metrics['edge_consistency_scores'])),
            'ssim_mean': float(np.mean(metrics['ssim_scores'])),
            'ssim_std': float(np.std(metrics['ssim_scores'])),
            'psnr_mean': float(np.mean(metrics['psnr_scores'])),
            'psnr_std': float(np.std(metrics['psnr_scores'])),
            'total_samples': successful_samples,
            'total_time_minutes': (time.time() - start_time) / 60
        }
        
        # Save detailed results
        detailed_results = {
            'final_metrics': final_metrics,
            'individual_results': results_log,
            'evaluation_settings': {
                'data_dir': data_dir,
                'output_dir': output_dir,
                'max_samples': max_samples,
                'steps': steps,
                'total_images_found': len(image_files),
                'successful_samples': successful_samples
            }
        }
        
        results_path = os.path.join(output_dir, 'evaluation_results.json')
        with open(results_path, 'w') as f:
            json.dump(detailed_results, f, indent=2)
        
        # Print comprehensive summary
        self.print_results_summary(final_metrics, results_path)
        
        return final_metrics

    def print_results_summary(self, metrics, results_path):
        """Print comprehensive results summary"""
        print("\n" + "="*80)
        print("üéØ INPAINTING MODEL EVALUATION RESULTS")
        print("="*80)
        
        print(f"üìä BASIC STATISTICS:")
        print(f"   Total Samples Evaluated: {metrics['total_samples']}")
        print(f"   Total Evaluation Time: {metrics['total_time_minutes']:.1f} minutes")
        print(f"   Average Time per Sample: {metrics['total_time_minutes']/metrics['total_samples']:.1f} minutes")
        
        print(f"\nüéØ CORE METRICS:")
        print(f"   FID Score (‚Üì lower=better): {metrics['fid_score']:.4f}")
        print(f"   LPIPS (‚Üì lower=better): {metrics['lpips_mean']:.4f} ¬± {metrics['lpips_std']:.4f}")
        print(f"   Edge Consistency (‚Üë higher=better): {metrics['edge_consistency_mean']:.4f} ¬± {metrics['edge_consistency_std']:.4f}")
        print(f"   SSIM (‚Üë higher=better): {metrics['ssim_mean']:.4f} ¬± {metrics['ssim_std']:.4f}")
        print(f"   PSNR (‚Üë higher=better): {metrics['psnr_mean']:.2f} ¬± {metrics['psnr_std']:.2f}")
        
        print(f"\nüìã INTERPRETATION:")
        
        # FID interpretation
        if metrics['fid_score'] < 30:
            fid_quality = "üåü Excellent (state-of-the-art)"
        elif metrics['fid_score'] < 50:
            fid_quality = "‚úÖ Very Good"
        elif metrics['fid_score'] < 100:
            fid_quality = "üëç Good"
        elif metrics['fid_score'] < 200:
            fid_quality = "‚ö†Ô∏è  Fair"
        else:
            fid_quality = "‚ùå Poor"
        print(f"   Overall Image Quality: {fid_quality}")
        
        # LPIPS interpretation
        if metrics['lpips_mean'] < 0.1:
            lpips_quality = "üåü Excellent perceptual quality"
        elif metrics['lpips_mean'] < 0.2:
            lpips_quality = "‚úÖ Good perceptual quality"
        elif metrics['lpips_mean'] < 0.4:
            lpips_quality = "üëç Fair perceptual quality"
        else:
            lpips_quality = "‚ùå Poor perceptual quality"
        print(f"   Perceptual Quality: {lpips_quality}")
        
        # Edge consistency interpretation
        if metrics['edge_consistency_mean'] > 0.8:
            edge_quality = "üåü Excellent edge blending"
        elif metrics['edge_consistency_mean'] > 0.6:
            edge_quality = "‚úÖ Good edge blending"
        elif metrics['edge_consistency_mean'] > 0.4:
            edge_quality = "üëç Fair edge blending"
        else:
            edge_quality = "‚ùå Poor edge blending"
        print(f"   Edge Consistency: {edge_quality}")
        
        # SSIM interpretation
        if metrics['ssim_mean'] > 0.9:
            ssim_quality = "üåü Excellent structural preservation"
        elif metrics['ssim_mean'] > 0.7:
            ssim_quality = "‚úÖ Good structural preservation"
        elif metrics['ssim_mean'] > 0.5:
            ssim_quality = "üëç Fair structural preservation"
        else:
            ssim_quality = "‚ùå Poor structural preservation"
        print(f"   Structural Similarity: {ssim_quality}")
        
        print(f"\nüìÅ DETAILED RESULTS SAVED TO:")
        print(f"   {results_path}")
        print("="*80)


def main():
    parser = argparse.ArgumentParser(description="Complete Inpainting Model Evaluation")
    parser.add_argument("--data_dir", type=str, default='data/inpainting_dataset',
                        help="Directory containing extracted dataset (images/, masks/, etc.)")
    parser.add_argument("--output_dir", type=str, default='evaluation/inpainting_real',
                        help="Directory to save evaluation results")
    parser.add_argument("--config", type=str, default="models/ldm/inpainting_big/config.yaml",
                        help="Path to model config")
    parser.add_argument("--model", type=str, default="models/ldm/inpainting_big/last.ckpt",
                        help="Path to model checkpoint")
    parser.add_argument("--max_samples", type=int, default=5,
                        help="Maximum number of samples to evaluate (for testing)")
    parser.add_argument("--steps", type=int, default=100,
                        help="Number of DDIM sampling steps")
    
    args = parser.parse_args()
    
    print("üé® COMPLETE INPAINTING MODEL EVALUATION")
    print("="*60)
    print(f"üìÅ Data Directory: {args.data_dir}")
    print(f"üìÅ Output Directory: {args.output_dir}")
    print(f"‚öôÔ∏è  Model Config: {args.config}")
    print(f"‚öôÔ∏è  Model Checkpoint: {args.model}")
    print(f"üìä Max Samples: {args.max_samples or 'All'}")
    print(f"üîÑ DDIM Steps: {args.steps}")
    
    # Check if CUDA is available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üíª Using Device: {device}")
    
    # Check required files
    required_files = [args.config, args.model]
    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print(f"‚ùå Missing required files: {missing_files}")
        sys.exit(1)
    
    if not os.path.exists(args.data_dir):
        print(f"‚ùå Data directory not found: {args.data_dir}")
        sys.exit(1)
    
    print("‚úÖ All required files found")
    
    try:
        # Initialize evaluator
        evaluator = InpaintingEvaluator(args.config, args.model, device)
        
        # Run evaluation
        print(f"\nüöÄ Starting evaluation...")
        metrics = evaluator.evaluate_dataset(
            data_dir=args.data_dir,
            output_dir=args.output_dir,
            max_samples=args.max_samples,
            steps=args.steps
        )
        
        if metrics:
            print("\nüéâ Evaluation completed successfully!")
        else:
            print("\n‚ùå Evaluation failed!")
            
    except Exception as e:
        print(f"\n‚ùå Evaluation failed with error: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()